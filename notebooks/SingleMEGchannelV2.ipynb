{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Imports\n",
      "======="
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#disable autosave functionality;\n",
      "#This script is taxing enough, and autosaving tends to push it over the edge\n",
      "# plus, autosaving seems to zero out the file before restoring it from the backup\n",
      "# this means an autosave causing a crash will actually delete the file rather than saving it!!!\n",
      "%autosave 0"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#basic imports\n",
      "\n",
      "%pylab inline\n",
      "import time\n",
      "import pickle\n",
      "import logging as L\n",
      "L.basicConfig(level=L.ERROR) # INFO)\n",
      "import time\n",
      "import numpy\n",
      "#import #pylab\n",
      "import scipy.stats\n",
      "import os\n",
      "import pylab\n",
      "import sklearn\n",
      "import scipy\n",
      "import sklearn.linear_model\n",
      "import re\n",
      "\n",
      "pylab.rcParams['figure.figsize'] = 10,10 #change the default image size for this session\n",
      "pylab.ion()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#custom imports\n",
      "%cd ../scripts\n",
      "\n",
      "# brian's prototype routines\n",
      "from protoMEEGutils import *\n",
      "import protoSpectralWinFFTMapper as specfft"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Definitions\n",
      "==========="
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# OUTLINE, 19th Nov 2014\n",
      "#\n",
      "# script for initial \"signs of life\" analysis of single MEG\n",
      "#\n",
      "# load in a meg file in EEGlab format\n",
      "# load in the word properties\n",
      "# choose a \"languagey\" channel (left-temporal, clean, with expected ERF patterns) \n",
      "# plot some ERFs (e.g. all nouns vs all preps) as sanity check\n",
      "# do the earlier analysis of R^2 and betas due to lexical access vars, up to X words back \n",
      "# hand over to Marten, so he can do the analysis based on syntactic embedding...\n",
      "\n",
      "\n",
      "#### SUBROUTINES ####\n",
      "\n",
      "# plot the time-scale for ERFs and other epoch figures \n",
      "def commonPlotProps():\n",
      "        #zeroSample = (abs(epochStart)/float(epochLength)*epochNumTimepoints)\n",
      "        #pylab.plot((0,epochNumTimepoints),(0,0),'k--')\n",
      "        #pylab.ylim((-2.5e-13,2.5e-13)) #((-5e-14,5e-14)) # better way just to get the ones it choose itself?\n",
      "        #pylab.plot((zeroSample,zeroSample),(0,0.01),'k--')\n",
      "        pylab.xticks(numpy.linspace(0,epochNumTimepoints,7),epochStart+(numpy.linspace(0,epochNumTimepoints,7)/samplingRate))\n",
      "        pylab.xlabel('time (s) relative to auditory onset') #+refEvent)\n",
      "        pylab.xlim((62,313))\n",
      "        pylab.show()\n",
      "        pylab.axhline(0, color='k', linestyle='--')\n",
      "        pylab.axvline(125, color='k', linestyle='--')\n",
      "\n",
      "# plot the time-scale for ERFs and other epoch figures \n",
      "def commonPlotProps():\n",
      "        #zeroSample = (abs(epochStart)/float(epochLength)*epochNumTimepoints)\n",
      "        #pylab.plot((0,epochNumTimepoints),(0,0),'k--')\n",
      "        #pylab.ylim((-2.5e-13,2.5e-13)) #((-5e-14,5e-14)) # better way just to get the ones it choose itself?\n",
      "        #pylab.plot((zeroSample,zeroSample),(0,0.01),'k--')\n",
      "        pylab.xticks(numpy.linspace(0,epochNumTimepoints,7),epochStart+(numpy.linspace(0,epochNumTimepoints,7)/samplingRate))\n",
      "        pylab.xlabel('time (s) relative to auditory onset') #+refEvent)\n",
      "        pylab.xlim((62,313))\n",
      "        pylab.show()\n",
      "        pylab.axhline(0, color='k', linestyle='--')\n",
      "        pylab.axvline(125, color='k', linestyle='--')\n",
      "        \n",
      "# adjust R2 down for the artificial inflation you get by increasing the number of explanatory features\n",
      "def adjustR2(R2, numFeatures, numSamples):\n",
      "        #1/0\n",
      "        #return R2\n",
      "        return R2-(1-R2)*(float(numFeatures)/(numSamples-numFeatures-1))\n",
      "\n",
      "# normalise (z-scale) the scale of variables (for the explanatory ones, so the magnitude of beta values are comparably interpretable)\n",
      "def mynormalise(A):\n",
      "        A = scipy.stats.zscore(A)\n",
      "        A[numpy.isnan(A)] = 0\n",
      "        return A"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Preprocessing\n",
      "============="
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Input Params\n",
      "----------"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#change to the data directory to load in the data\n",
      "%cd ../MEG_data\n",
      "\n",
      "#*# MARTY #*# choose a file - I found participant V to be pretty good, and 0.01 to 50Hz filter is pretty conservative #*#\n",
      "(megFileTag1, megFile1) = ('V_TSSS_0.01-50Hz_@125', 'v_hod_allRuns_tsss_audiobookPrepro_stPad1_lp50_resamp125_frac10ICAed.set')#_hp0.010000.set')\n",
      "(megFileTag2, megFile2) = ('A_TSSS_0.01-50Hz_@125', 'aud_hofd_a_allRuns_tsss_audiobookPrepro_stPad1_lp50_resamp125_frac10ICAed_hp0.010000.set')\n",
      "(megFileTag3, megFile3) = ('C_TSSS_0.01-50Hz_@125', 'aud_hofd_c_allRuns_tsss_audiobookPrepro_stPad1_lp50_resamp125_frac10ICAed_hp0.010000.set')\n",
      "\n",
      "## put your on properties in here, as a .tab file of similar format (tab delimited, and field names in a first comment line - should be easy to do in excel...)\n",
      "#to get the V5.tab:\n",
      "#  python ../scripts/buildtab.py hod_JoeTimes_LoadsaFeaturesV3.tab hod.wsj02to21-comparativized-gcg15-1671-4sm.fullberk.parsed.gcgbadwords > hod_JoeTimes_LoadsaFeaturesV4.tab\n",
      "#  python ../scripts/addsentid.py hod_JoeTimes_LoadsaFeaturesV4.tab > hod_JoeTimes_LoadsaFeaturesV5.tab\n",
      "tokenPropsFile = 'hod_JoeTimes_LoadsaFeaturesV5.tab' # best yet! have automatic Stanford tagging, several word length and freq measures, and also the 2 and 3 token back-grams\n",
      "\n",
      "# WHICH CHANNELS TO LOOK AT AS ERFS\n",
      "#*# MARTY #*# decide which channels to use - channels of interest are the first few you can look at in an ERF, and then from them you can choose one at a time with \"channelToAnalyse\" for the actual regression analysis #*#\n",
      "channelLabels = ['MEG0111', 'MEG0121', 'MEG0131', 'MEG0211', 'MEG0212', 'MEG0213', 'MEG0341']\n",
      "#?# this way of doing things was a slightly clumsy work-around, cos I didn't have enough memory to epoch all 306 channels at one time\n",
      "\n",
      "# LOAD WORD PROPS\n",
      "#*# MARTY #*# change dtype to suit the files in your .tab file #*#\n",
      "tokenProps = scipy.genfromtxt(tokenPropsFile,\n",
      "                              delimiter='\\t',names=True,\n",
      "                              dtype=\"i4,f4,f4,S50,S50,i2,i2,i2,S10,f4,f4,f4,f4,f4,f4,f4,f4,f4,f4,f4,i1,>i4\")\n",
      "# ... and temporarily save as cpickle archive to satisfy the way I programmed the convenience function loadBookMEGWithAudio (it expects to find the same info in a C-pickle file, and so doesn't need to know about number and type of fields)\n",
      "tokenPropsPickle = tokenPropsFile+'.cpk'\n",
      "cPickle.dump(tokenProps, open(tokenPropsPickle, 'wb'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Trial Params\n",
      "------------"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "triggersOfInterest=['s%d' % i for i in range(1,10)]\n",
      "refEvent = 'onTime' #,'offTime']\n",
      "#*# MARTY #*# guess an epoch of -0.5 to +1s should be enough #*#\n",
      "epochStart = -1; # stimulus ref event\n",
      "epochEnd = +2; # \n",
      "epochLength = epochEnd-epochStart;\n",
      "baseline = False #[-1,0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Epoch Data\n",
      "----------"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Get the goods on subject 1\n",
      "(contSignalData1, metaData1, trackTrials, tokenProps, audioSignal, samplingRate, numChannels) = loadBookMEGWithAudio(megFile1, tokenPropsPickle, triggersOfInterest, epochEnd, epochStart, icaComps=False)\n",
      "\n",
      "channelsOfInterest = [i for i in range(len(metaData1.chanlocs)) if metaData1.chanlocs[i].labels in channelLabels]\n",
      "\n",
      "severalMagChannels1 = contSignalData1[channelsOfInterest,:]\n",
      "(wordTrials1, epochedSignalData1, epochSliceTimepoints, wordTimesAbsolute, numTrials, epochNumTimepoints) = wordTrialEpochify(severalMagChannels1, samplingRate, tokenProps, trackTrials, refEvent, epochEnd, epochStart)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "del contSignalData1\n",
      "del severalMagChannels1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Get the goods on subject 2\n",
      "(contSignalData2, metaData2, trackTrials, tokenProps, audioSignal, samplingRate, numChannels) = loadBookMEGWithAudio(megFile2, tokenPropsPickle, triggersOfInterest, epochEnd, epochStart, icaComps=False)\n",
      "severalMagChannels2 = contSignalData2[channelsOfInterest,:]\n",
      "(wordTrials2, epochedSignalData2, epochSliceTimepoints, wordTimesAbsolute, numTrials, epochNumTimepoints) = wordTrialEpochify(severalMagChannels2, samplingRate, tokenProps, trackTrials, refEvent, epochEnd, epochStart)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "del contSignalData2\n",
      "del severalMagChannels2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Get the goods on subject 3\n",
      "(contSignalData3, metaData3, trackTrials, tokenProps, audioSignal, samplingRate, numChannels) = loadBookMEGWithAudio(megFile3, tokenPropsPickle, triggersOfInterest, epochEnd, epochStart, icaComps=False)\n",
      "severalMagChannels3 = contSignalData3[channelsOfInterest,:]\n",
      "(wordTrials3, epochedSignalData3, epochSliceTimepoints, wordTimesAbsolute, numTrials, epochNumTimepoints) = wordTrialEpochify(severalMagChannels3, samplingRate, tokenProps, trackTrials, refEvent, epochEnd, epochStart)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "del contSignalData3\n",
      "del severalMagChannels3"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "epochedSignalData = numpy.concatenate((epochedSignalData1,epochedSignalData2,epochedSignalData3), axis=0)\n",
      "print(epochedSignalData.shape)\n",
      "tokenProps = numpy.concatenate((tokenProps,tokenProps,tokenProps),axis=0)\n",
      "print(tokenProps.shape)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Plots for Sanity check\n",
      "-----------------"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#pylab.figure()\n",
      "#pylab.subplot(2,1,1)\n",
      "#pylab.plot(audioSignal)\n",
      "#pylab.title('audio signal')\n",
      "\n",
      "#pylab.subplot(2,1,2)\n",
      "#pylab.plot(contSignalData[0,:])\n",
      "#pylab.title('first MEG signal')\n",
      "\n",
      "#pylab.figure()\n",
      "#pylab.title('ERF over all tokens, selected channels')\n",
      "#pylab.plot( numpy.mean(epochedSignalData,axis=0).T)\n",
      "#pylab.legend(channelLabels, loc=4)\n",
      "#commonPlotProps()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Analysis\n",
      "=========="
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Remove undesired trials\n",
      "----------------------"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# REDUCE TRIALS TO JUST THOSE THAT CONTAIN A REAL WORD (NOT PUNCTUATION, SPACES, ...)\n",
      "wordTrialsBool = numpy.array([p != '' for p in tokenProps['stanfPOS']])\n",
      "print(wordTrialsBool[:10])\n",
      "# REDUCE TRIALS TO JUST THOSE THAT HAVE A DECENT DEPTH ESTIMATE\n",
      "parsedTrialsBool = numpy.array([d != -1 for d in tokenProps['syndepth']])\n",
      "print(parsedTrialsBool[:10])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Set up the dev and test sets\n",
      "devsizerecip = 3 # the reciprocal of the dev size, so devsizerecip = 3 means the dev set is 1/3 and the test set is 2/3\n",
      "devitems = numpy.arange(1,max(tokenProps['sentid']),devsizerecip)\n",
      "devTrialsBool = numpy.array([s in devitems for s in tokenProps['sentid']])\n",
      "testTrialsBool = numpy.array([s not in devitems for s in tokenProps['sentid']])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Select dataset\n",
      "-----------"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "inDataset = devTrialsBool"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print tokenProps.shape,epochedSignalData.shape\n",
      "wordEpochs = epochedSignalData[wordTrialsBool & parsedTrialsBool & inDataset]\n",
      "wordFeatures = tokenProps[wordTrialsBool & parsedTrialsBool & inDataset]\n",
      "print wordFeatures.shape, wordEpochs.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Spectral decomposition\n",
      "---------------------"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#test reshape outcomes\n",
      "a = np.arange(18).reshape((3,2,3))\n",
      "print(a) #target: 3 epochs, 2 channels, 3 frequency_features\n",
      "b = np.arange(18).reshape((3,6))\n",
      "print(b) #fft output: 3 epochs, 2 channels x 3 frequency_features\n",
      "c = b.reshape((3,2,3))\n",
      "print(c) #reshaped output: 3 epochs, 2 channels, 3 frequency_features"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# The FFT script collapses across channels\n",
      "# index0: epoch\n",
      "# index1: channels x fft_feature_types x frequencies\n",
      "# solution: reshape the output as (epochs,channels,-1)\n",
      "\n",
      "print 'wordEpochs: ',wordEpochs.shape\n",
      "# Spectrally decompose the epochs\n",
      "(mappedTrialFeatures, spectralFrequencies) = specfft.mapFeatures(wordEpochs,samplingRate,windowShape='hann',featureType='amp')\n",
      "# Reshape output to get epochs x channels x frequency\n",
      "mappedTrialFeatures = mappedTrialFeatures.reshape((wordEpochs.shape[0],wordEpochs.shape[1],-1))\n",
      "print 'FFT output: ', mappedTrialFeatures.shape, spectralFrequencies.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# The FFT script collapses across channels\n",
      "# index0: epoch\n",
      "# index1: channels x fft_feature_types x frequencies\n",
      "# solution: reshape the output as epochs,channels,-1\n",
      "\n",
      "#print 'wordEpochs: ',wordEpochs.shape\n",
      "#create a dummy array to permit looping\n",
      "#mappedTrialFeatures = numpy.zeros((wordEpochs.shape[0],1,wordEpochs.shape[2]))\n",
      "#FIRST = True\n",
      "#for i in range(wordEpochs.shape[1]):\n",
      "    #for each channel, get a spectral decomposition of it\n",
      "    #if FIRST:\n",
      "    #    print 'dummy: ', mappedTrialFeatures.shape, '\\n'\n",
      "    #else:\n",
      "    #    print 'real:  ', mappedTrialFeatures.shape, '\\n'\n",
      "\n",
      "    #singleChannelEpochs = wordEpochs[:,i,:].reshape(wordEpochs.shape[0],1,-1)\n",
      "    ##(mappedTrialFeaturesRun, spectralFrequencies) = specfft.mapFeatures(singleChannelEpochs,samplingRate,windowShape='hann',featureType='amp')\n",
      "    #mappedTrialFeaturesRun = singleChannelEpochs[:,:,:-1]\n",
      "\n",
      "    #print 'run:   ',mappedTrialFeaturesRun.shape\n",
      "    #mappedTrialFeaturesRun = mappedTrialFeaturesRun.reshape(mappedTrialFeaturesRun.shape[0],1,-1)\n",
      "    #if FIRST:\n",
      "    #    mappedTrialFeatures = numpy.copy(mappedTrialFeaturesRun)\n",
      "    #else:\n",
      "    #    mappedTrialFeatures = numpy.concatenate((mappedTrialFeatures,mappedTrialFeaturesRun),axis=1)\n",
      "    #FIRST = False\n",
      "#print 'final: ',mappedTrialFeatures.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(spectralFrequencies)\n",
      "freqsource = None #Can be 'weiss', 'wiki', or an interpolation of the two\n",
      "if freqsource == 'weiss':\n",
      "    #Weiss et al. 05\n",
      "    theta = numpy.nonzero( (spectralFrequencies >= 4) & (spectralFrequencies <= 7) )\n",
      "    beta1 = numpy.nonzero( (spectralFrequencies >= 13) & (spectralFrequencies <= 18) )\n",
      "    beta2 = numpy.nonzero( (spectralFrequencies >= 20) & (spectralFrequencies <= 28) )\n",
      "    gamma = numpy.nonzero( (spectralFrequencies >= 30) & (spectralFrequencies <= 34) )\n",
      "elif freqsource == 'wiki':\n",
      "    # end of http://en.wikipedia.org/wiki/Theta_rhythm\n",
      "    delta = numpy.nonzero( (spectralFrequencies >= 0.1) & (spectralFrequencies <= 3) )\n",
      "    theta = numpy.nonzero( (spectralFrequencies >= 4) & (spectralFrequencies <= 7) )\n",
      "    alpha = numpy.nonzero( (spectralFrequencies >= 8) & (spectralFrequencies <= 15) )\n",
      "    beta = numpy.nonzero( (spectralFrequencies >= 16) & (spectralFrequencies <= 31) )\n",
      "    gamma = numpy.nonzero( (spectralFrequencies >= 32) & (spectralFrequencies <= 100) )\n",
      "else:\n",
      "    #Interpolate between weiss and wiki\n",
      "    #print(numpy.nonzero((spectralFrequencies >= 4) & (spectralFrequencies <= 7)))\n",
      "    theta = numpy.nonzero( (spectralFrequencies >= 4) & (spectralFrequencies <= 7) )\n",
      "    alpha = numpy.nonzero( (spectralFrequencies >= 8) & (spectralFrequencies < 13) )\n",
      "    beta1 = numpy.nonzero( (spectralFrequencies >= 13) & (spectralFrequencies <= 18) )\n",
      "    beta2 = numpy.nonzero( (spectralFrequencies >= 20) & (spectralFrequencies <= 28) )\n",
      "    gamma = numpy.nonzero( (spectralFrequencies >= 30) & (spectralFrequencies <= 34) )\n",
      "print(theta)\n",
      "print(theta[0])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Select channel for analysis\n",
      "--------------"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(channelLabels)\n",
      "channelToAnalyse = 3 # index of the channels above to actually run regression analysis on\n",
      "print 'Analyzing:', channelLabels[channelToAnalyse]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Run Regression Analysis\n",
      "---------------------------"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# REGULARISATION VALUES TO TRY (e.g. in Ridge GCV)\n",
      "regParam = [0.001, 0.002, 0.005, 0.01, 0.02, 0.05, 0.1, 0.2, 0.5, 1, 2, 5, 10, 20, 50, 1e+2, 2e+2, 5e+2, 1e+3, 2e+3, 5e+3]\n",
      "\n",
      "# SELECT AND DESCRIBE THE REGRESSORS WE'RE CHOOSING TO USE\n",
      "# this strings should match the names of the fields in tokenProps\n",
      "#*# MARTY #*# here you should list the features you're choosing from your .tab file (just one?) #*#\n",
      "features = [\n",
      " #'logFreq_ANC',\n",
      " #'surprisal2back_COCA',\n",
      " #'bigramEntropy_COCA_here',\n",
      " 'syndepth'\n",
      "]\n",
      "#*# MARTY #*# ... this has shorthand versions of the variable names, for display, and also has to include the \"position\" one that this version of the script inserts by default #*#\n",
      "labelMap = {\n",
      " #'logFreq_ANC': 'freq',\n",
      " #'surprisal2back_COCA': 'surprisal',\n",
      " #'bigramEntropy_COCA_here': 'entropy',\n",
      " #'sentenceSerial': 'position',\n",
      " 'syndepth': 'depth'\n",
      "}\n",
      "legendLabels = features"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# SLOT REGRESSORS IN ONE BY ONE\n",
      "explanatoryFeatures = numpy.zeros((wordFeatures.shape)) # dummy\n",
      "#explanatoryFeatures = numpy.array([])\n",
      "for feature in features:\n",
      "        print feature\n",
      "        explanatoryFeatures = numpy.vstack((explanatoryFeatures, wordFeatures[feature]))\n",
      "explanatoryFeatures = explanatoryFeatures[1:].T # strip zeros out again\n",
      "\n",
      "# PLOT EFFECTS X EPOCHS BACK\n",
      "#*# MARTY #*# I guess you don't want to do the history thing (though is good initially for sanity check), so can leave this at 0 #*#\n",
      "epochHistory = 0"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "modelTrainingFit = []\n",
      "modelTestCorrelation = []\n",
      "modelParameters = []\n",
      "legendLabels = features\n",
      "#tmpFeatures = explanatoryFeatures.copy()\n",
      "#tmpLegend = legendLabels[:]\n",
      "#for epochsBack in range(1,epochHistory+1):\n",
      "#        epochFeatures = numpy.zeros(tmpFeatures.shape)\n",
      "#        epochFeatures[epochsBack:,:] = tmpFeatures[:-epochsBack,:]\n",
      "#        explanatoryFeatures = numpy.hstack((explanatoryFeatures,epochFeatures))\n",
      "#        legendLabels = legendLabels + [l+'-'+str(epochsBack) for l in tmpLegend]\n",
      "\n",
      "## put in sentence serial - can't leave in history, cos is too highly correlated across history...\n",
      "#explanatoryFeatures = numpy.vstack((explanatoryFeatures.T, wordFeatures['sentenceSerial'])).T\n",
      "#features.append('sentenceSerial')\n",
      "#legendLabels.append('sentenceSerial')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# STEP THROUGH EACH TIME POINT IN THE EPOCH, RUNNING REGRESSION FOR EACH ONE\n",
      "for t in theta[0]:#range(1):\n",
      "        #print 'fitting at timepoint',t\n",
      "        # NOTES # tried a load of different versions, and straight linear regression does as well as any of them, measured in terms of R^2\n",
      "\n",
      "        # WHICH VARIETY OF REGRESSION TO USE?\n",
      "        #*# MARTY #*# I get pretty similar results with all three of those below. The most generic (ie fewest extra assumptions) is normal LinearRegression. I guess RidgeCV should do best in terms of R^2, but has discontinuities in betas, as different regularisation parameters are optimal at each time step. LassoLars is something of a compromise. #*#\n",
      "        #lm = sklearn.linear_model.LinearRegression(fit_intercept=True, normalize=True)\n",
      "        #lm = sklearn.linear_model.RidgeCV(fit_intercept=True, normalize=True, alphas=regParam) #, 10000, 100000])\n",
      "        lm = sklearn.linear_model.LassoLars(alpha=0.0001) #(alpha=1.0, fit_intercept=True, verbose=False, normalize=True, precompute='auto', max_iter=500, eps=2.2204460492503131e-16, copy_X=True)\n",
      "\n",
      "        # NORMALISE THE EXPLANATORY VARIABLES? (for comparable beta magnitude interpretation)\n",
      "        #*# MARTY #*# choose whether to scale inputs #*#\n",
      "        trainX = mynormalise(explanatoryFeatures)\n",
      "        trainY = mynormalise(mappedTrialFeatures[:,channelToAnalyse,t])\n",
      "        #trainX = mynormalise(explanatoryFeatures)\n",
      "        #trainY = mynormalise(wordEpochs[:,channelToAnalyse,t])\n",
      "        #trainX = explanatoryFeatures\n",
      "        #trainY = wordEpochs[:,channelToAnalyse,t]\n",
      "\n",
      "        trainedLM = lm.fit(trainX,trainY)\n",
      "        modelParameters.append(lm)\n",
      "        #print(lm.score(trainX,trainY),trainX.shape[1], trainX.shape[0])\n",
      "        #modelTrainingFit.append(adjustR2(lm.score(trainX,trainY), trainX.shape[1], trainX.shape[0]))\n",
      "        modelTrainingFit.append(lm.score(trainX,trainY)) #for a single feature, no punishment is necessary"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(modelTrainingFit)\n",
      "print(numpy.sort(modelTrainingFit)[::-1])\n",
      "print 'ave fit: ', numpy.mean(modelTrainingFit)\n",
      "print 'max fit: ', numpy.max(modelTrainingFit)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Graph results\n",
      "============"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# DETERMINE IF THERE IS CORRELATION BETWEEN THE EXPLANATORY VARIABLES\n",
      "betaMatrix = numpy.array([p.coef_ for p in modelParameters])\n",
      "print(betaMatrix.shape)\n",
      "neatLabels = [l.replace(re.match(r'[^-]+',l).group(0), labelMap[re.match(r'[^-]+',l).group(0)]) for l in legendLabels if re.match(r'[^-]+',l).group(0) in labelMap]\n",
      "legendLabels = numpy.array(legendLabels)\n",
      "#numFeaturesDisplay = len(legendLabels)\n",
      "neatLabels = numpy.array(neatLabels)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# DO BIG SUMMARY PLOT OF FEATURE CORRELATIONS, R^2 OVER TIMECOURSE, BETAS OVER TIME COURSE, AND ERF/ERP\n",
      "f = pylab.figure(figsize=(10,10))\n",
      "s = pylab.subplot(2,2,1)\n",
      "pylab.title('R-squared '+str(trainedLM))\n",
      "pylab.plot(modelTrainingFit, linewidth=2)\n",
      "commonPlotProps()\n",
      "s = pylab.subplot(2,2,2)\n",
      "if betaMatrix.shape[1] > 7:\n",
      "        pylab.plot(betaMatrix[:,:7], '-', linewidth=2)\n",
      "        pylab.plot(betaMatrix[:,7:], '--', linewidth=2)\n",
      "else:\n",
      "        pylab.plot(betaMatrix, '-', linewidth=2)\n",
      "\n",
      "pylab.legend(neatLabels)\n",
      "#pylab.legend(legendLabels)\n",
      "pylab.title('betas for all (normed) variables')\n",
      "commonPlotProps()\n",
      "\n",
      "\n",
      "s = pylab.subplot(3,3,2)\n",
      "pylab.title('correlations between explanatory variables')\n",
      "pylab.imshow(numpy.abs(numpy.corrcoef(explanatoryFeatures.T)),interpolation='nearest', origin='upper') # leave out the dummy one\n",
      "pylab.clim(0,1)\n",
      "pylab.yticks(range(len(neatLabels)),neatLabels)\n",
      "pylab.ylim((-0.5,len(neatLabels)-0.5))\n",
      "pylab.xticks(range(len(neatLabels)),neatLabels, rotation=90)\n",
      "pylab.xlim((-0.5,len(neatLabels)-0.5))\n",
      "pylab.colorbar()\n",
      "\n",
      "#fontP = FontProperties()\n",
      "#fontP.set_size('small')\n",
      "#legend([s], \"title\", prop = fontP)\n",
      "\n",
      "#s = pylab.subplot(2,2,4)\n",
      "#pylab.plot(numpy.mean(epochedSignalData[wordTrialsBool,channelToAnalyse],axis=0).T, linewidth=2)\n",
      "#pylab.title('ERF')\n",
      "#commonPlotProps()\n",
      "\n",
      "#print 'history %d, mean model fit over -0.5s to +1.0s: %.5f, max is %.5f' % (epochHistory, numpy.mean(modelTrainingFit[62:250]), numpy.max(modelTrainingFit[62:250]))\n",
      "#pylab.savefig('meg_testfig_%s.png' % (channelLabels[channelToAnalyse]))\n",
      "\n",
      "pylab.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}